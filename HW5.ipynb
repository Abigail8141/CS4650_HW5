{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4650_HW5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3jSUHDRTVD03GnGYLxqqg"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QMo-rh9FgOW"
      },
      "source": [
        "!pip install spacy\n",
        "!pip install newsapi-python\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ3EnWYNFpwP"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBg94YgmGcnb"
      },
      "source": [
        "import en_core_web_lg\n",
        "import spacy\n",
        "import pickle\n",
        "import numpy as np\n",
        "from newsapi import NewsApiClient\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nlp_eng = en_core_web_lg.load()\n",
        "newsapi = NewsApiClient (api_key='122ffe874e59412d8f5649f679292dd9')\n",
        "\n",
        "temp = newsapi.get_everything(q='coronavirus', language='en', from_param='2021-09-23', to='2021-10-21', sort_by='relevancy')\n",
        "\n",
        "filename = 'articlesCOVID.pckl'\n",
        "pickle.dump(temp, open(filename, 'wb'))\n",
        "\n",
        "filename = 'articlesCOVID.pckl'\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "filepath = '/content/articlesCOVID.pckl'\n",
        "pickle.dump(loaded_model, open(filepath, 'wb'))\n",
        "\n",
        "dados = []\n",
        "for i, article in enumerate(temp):\n",
        "    for x in temp['articles']:\n",
        "        title = x['title']\n",
        "        description = x['description']\n",
        "        content = x['content']\n",
        "        date = x['publishedAt']\n",
        "        dados.append({'title':title, 'date':date, 'desc':description, 'content':content})\n",
        "df = pd.DataFrame(dados)\n",
        "df = df.dropna()\n",
        "df.head()\n",
        "\n",
        "def get_keywords_eng(token):\n",
        "  result = []\n",
        "  punctuation = string.punctuation\n",
        "  stop_words = stopwords.words(\"english\")\n",
        "  for i in token:\n",
        "    if (i in stop_words):\n",
        "      continue\n",
        "    else:\n",
        "      result.append(i)\n",
        "  return result\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w +')\n",
        "results = []\n",
        "for content in df.content.values:\n",
        "    content = tokenizer.tokenize(content)\n",
        "    results.append([x[0] for x in Counter(get_keywords_eng(content)).most_common(5)])\n",
        "df['keywords'] = results\n",
        "\n",
        "text = str(results)\n",
        "wordcloud = WordCloud(max_font_size=30, max_words=150, background_color=\"white\").generate(text)\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}